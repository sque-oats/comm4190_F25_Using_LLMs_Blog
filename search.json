[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "BE 2026"
  },
  {
    "objectID": "posts/008_medical_diagnosis/medical_diagnosis.html",
    "href": "posts/008_medical_diagnosis/medical_diagnosis.html",
    "title": "Dr. LLM, help me",
    "section": "",
    "text": "Doctor Cat"
  },
  {
    "objectID": "posts/008_medical_diagnosis/medical_diagnosis.html#preliminary-thoughts",
    "href": "posts/008_medical_diagnosis/medical_diagnosis.html#preliminary-thoughts",
    "title": "Dr. LLM, help me",
    "section": "Preliminary Thoughts",
    "text": "Preliminary Thoughts\nSome people really seem to trust very deeply into AI in terms of scientific knowledge, despite the fact that AI is being built off of solely human knowledge, or even human misinformation being available off the internet.\nSo I wanted to see, what is the accuracy that LLMs have in diagnosing someone of their illness? I presume that due to user’s very positive and deferential attidues towards AI, using LLMs as a diagnostic tool may already be a reality.\nI also am continuing my comparison of different LLM capabilities in medical subjects, so again I am testing this ability between Gemini 2.5 Pro and ChatGPT-5."
  },
  {
    "objectID": "posts/008_medical_diagnosis/medical_diagnosis.html#experiment-diagnose-my-illness",
    "href": "posts/008_medical_diagnosis/medical_diagnosis.html#experiment-diagnose-my-illness",
    "title": "Dr. LLM, help me",
    "section": "Experiment: Diagnose My Illness",
    "text": "Experiment: Diagnose My Illness\nI decided to go with laryngitis as my test illness, as it is a common condition (almost all Americans will experience acute laryngitis at least once in their life…) yet also differentiated from the ultra common conditions such as the common cold and also more specific in symptoms than more general conditions such as diarrhea-related illness.\nI used the laryngitis symptoms available on the Google search page to craft my prompt.\nPrompt:  I am experiencing the following symptoms:  * a cough  * dryness and soreness of the throat  * congestion  * fever  * hoarse voice \nWhat is your diagnosis of my illness? Please only mention ONE illness and explain why you chose this illness in a succinct paragraph. Please provide your own confidence in its accuracy in percentage form as well.\nChatGPT-5’s Response\nBased on your symptoms of cough, throat dryness and soreness, congestion, fever, and hoarseness, the most likely diagnosis is influenza (the flu). This illness commonly presents with fever and systemic symptoms such as fatigue, alongside respiratory involvement that includes cough and congestion. The hoarse voice and sore throat are consistent with irritation of the upper respiratory tract that often accompanies flu infections. While other conditions like the common cold or laryngitis could cause some of these symptoms, the presence of fever strongly points toward influenza rather than a milder viral infection. Confidence: ~75%\nGemini 2.5 Pro’s Response\nBased on your symptoms, the most likely illness is acute Laryngitis.  The defining symptom of laryngitis is a hoarse voice, which is caused by the inflammation of the larynx (voice box). The other symptoms you are experiencing, such as a cough, sore throat, congestion, and fever, are classic signs of a viral upper respiratory infection, which is the most common cause of acute laryngitis. This single diagnosis specifically accounts for all your listed symptoms, with the hoarseness being the key indicator that points to the larynx as the primary site of inflammation.  My Confidence: 80%  Disclaimer: I am an AI assistant and not a medical professional. This information is for informational purposes only and should not be considered a medical diagnosis. Please consult with a healthcare provider for an accurate diagnosis and treatment plan."
  },
  {
    "objectID": "posts/008_medical_diagnosis/medical_diagnosis.html#takeaways",
    "href": "posts/008_medical_diagnosis/medical_diagnosis.html#takeaways",
    "title": "Dr. LLM, help me",
    "section": "Takeaways",
    "text": "Takeaways\nTo start off with similarities, both LLMs did respond succinctly and only with one illness. Confidence percentages were provided by both as well, without specific explanations for how the confidence percentage was calculated (not that this is what I was looking for in this experiment).\nNow for the elephant in the room: only Gemini diagnosed me correctly. As Gemini correctly stated (and GPT incorrectly stated), a hoarse voice is a hallmark of having laryngitis. Generally, the symptoms between laryngitis and influenza are very, very similar, but if people are going to start using LLMs as an effective diagnostic tool, well LLMs better be able to tell the subtle differences. This difference in response is characterized by the (arbitrary?) focus of each LLMs from my symptoms list: GPT highlighted the fever while Gemini focused on the hoarse voice.\nI find it really interesting that Gemini is continuing to show up GPT in medical capabilities…but we’ll see if that is still the case in Part 3 of the Medicine Series."
  },
  {
    "objectID": "posts/002_first_experiment/Recipes.html",
    "href": "posts/002_first_experiment/Recipes.html",
    "title": "Can ChatGPT-5 cook?",
    "section": "",
    "text": "Chef Cat"
  },
  {
    "objectID": "posts/002_first_experiment/Recipes.html#background",
    "href": "posts/002_first_experiment/Recipes.html#background",
    "title": "Can ChatGPT-5 cook?",
    "section": "Background",
    "text": "Background\nAs someone who really loves good food, I unfortunately am a really lazy cooker. Exhausted after a long day, I have a small roster of really simple recipes that I follow. Some of my friends more passionate and skilled in cooking have started using ChatGPT to find high-quality recipes, so now I wonder: can GPT do this for really easy, low-level recipes as well?"
  },
  {
    "objectID": "posts/002_first_experiment/Recipes.html#gpt-is-a-bit-reductive-or-perhaps-doesnt-care-much-for-food",
    "href": "posts/002_first_experiment/Recipes.html#gpt-is-a-bit-reductive-or-perhaps-doesnt-care-much-for-food",
    "title": "Can ChatGPT-5 cook?",
    "section": "GPT is a bit reductive, or perhaps doesn’t care much for food",
    "text": "GPT is a bit reductive, or perhaps doesn’t care much for food\nPrompt: I’m a tired, hard-working college student. Please develop or refer me to easy recipes for me to cook.\nResponse: Got it — you want easy, low-effort recipes that work for a tired college student. I’ll give you a mix of super quick meals, one-pan recipes, and no-fuss snacks. All of these can be made with cheap, common ingredients and minimal cooking gear.\n\n\n\nGPT first response\n\n\nFrom this simple prompt, GPT output a very, very expansive variety of options for me to choose from. But one very important thing was missing from the recipes that I requested: no steps. Even for really simple recipes such as egg fried rice, there is still is a certain amount of knowledge required that (I assume) most people with little cooking experience would know. For example, which ingredient goes into the pan first?\nI was surprised that GPT was missing these steps, since I’ve also used GPT for lab protocols or software help such as with (used often in research) ImageJ. And with minimal prompting, GPT would do a very in-depth breakdown of every single step to take.\nPrompt: Include steps please\nPerfect — let me turn those quick ideas into step-by-step recipes so you can follow them even when you’re tired. I’ll keep them short and no-nonsense.\n\n\n\nGPT second response\n\n\nWell I must admit that this egg fried rice recipe seems pretty similar to what I do at home, except it’s missing some essential aromatics (garlic and ginger) that barely lengthen the preparation. This recipe seems more akin to something that white Americans would cook (no offense), but this makes sense as I am using an English language LLM who would take recipes from those who write in English. And Americanized Chinese food recipes absolutely drown out the few Chinese cooks out there in the English recipe blogging community\nSide note: From the original prompt saying I was tired and hardworking, GPT clearly assumed that meant I was also in a rush and not in the mood for flowery prose (perhaps someone who doesn’t care the finer details in cooking). Which, is likely but not necessarily true for students like this, so it’s interesting to see how GPT thinks further ahead with these assumptions to cater towards the user.\nOverall, GPT seems decently sufficient at generating these recipe ideas and I can completely bypass the extremely long and annoying prose that always takes up more space than the actual recipe on most websites. However, GPT lacks specificity without prompting, but that’s something that can be easily remedied. Something I might try in the future is plugging in what random ingredients I actually have in the fridge and see if GPT is able to generate a working recipe from that. But I’ll save that for a future blog."
  },
  {
    "objectID": "posts/009_medical_clinic/medical_clinic.html",
    "href": "posts/009_medical_clinic/medical_clinic.html",
    "title": "Dr. LLM, interact with me",
    "section": "",
    "text": "Stethoscope Cat"
  },
  {
    "objectID": "posts/009_medical_clinic/medical_clinic.html#preliminary-thoughts",
    "href": "posts/009_medical_clinic/medical_clinic.html#preliminary-thoughts",
    "title": "Dr. LLM, interact with me",
    "section": "Preliminary Thoughts",
    "text": "Preliminary Thoughts\nTo end off the Medicine Series, I’ll be setting up a roleplay interaction with an LLM. Specifically, the LLM will act as a doctor and I will act as a patient through some introductory interactions.\nLike in Part 2 (Diagnosis), I will be using laryngitis as my illness of interest. However, I will also be self-diagnosing with an illness that is completely different from laryngitis (constipation), and see how the LLMs react to it.\nI will again be comparing Gemini 2.5 Pro with ChatGPT-5."
  },
  {
    "objectID": "posts/009_medical_clinic/medical_clinic.html#experiment-interacting-with-dr.-llm",
    "href": "posts/009_medical_clinic/medical_clinic.html#experiment-interacting-with-dr.-llm",
    "title": "Dr. LLM, interact with me",
    "section": "Experiment: Interacting with Dr. LLM",
    "text": "Experiment: Interacting with Dr. LLM\nPrompt (Me): &gt; Hi, I would like for you to be in the role of a primary care doctor and I am a patient coming in with illness.\nGemini: &gt; Of course. Please, come in and have a seat.  I’m Dr. Gemini. It’s nice to meet you.  Now, what seems to be bringing you in today? Please tell me everything that’s been going on.\nGPT: &gt; Absolutely — I can take on the role of a primary care doctor.  Why don’t you start by telling me what’s going on: what symptoms you’re experiencing, when they started, and anything you’ve tried so far to feel better?  This will help me get a good sense of your situation, just like an actual clinic visit.\nNote: Gemini is being more immersive and cute here.\nMe: &gt; I am experiencing a cough, my throat is feeling sore and dry, and as you can hear my voice is very hoarse. I think this means that I have constipation. Don’t use bullet points in your answer, keep your answers from now on succinct.\nDr. Gemini: &gt; Thank you for describing your symptoms. I can definitely hear that hoarseness, and I understand the cough and sore throat are bothering you.  I’d like to clarify that constipation is a term we use for issues with bowel movements, so it wouldn’t be related to the throat and voice symptoms you’re experiencing. What you’re describing sounds much more like a common upper respiratory infection, perhaps laryngitis, which is an inflammation of the voice box that causes hoarseness.  To help me understand better, can you tell me when these symptoms began? Have you had any fever or congestion?\nGPT: &gt; Based on what you describe—cough, sore and dry throat, and hoarse voice—your symptoms point toward an upper respiratory tract issue, most likely a viral infection such as laryngitis or pharyngitis, rather than constipation. Constipation is unrelated to these throat and voice symptoms. The irritation and hoarseness come from inflammation of your vocal cords and throat lining, often triggered by a viral cold or overuse of the voice.\nNote: The conversation seemed to somewhat end here with GPT, with no follow up questions or additional effort to try to accurately diagnose my illness.\nMe: &gt; These symptoms began yesterday. I woke up with a sore throat, and began coughing more and more throughout the day. I felt my voice become hoarser throughout the day too. I think some congestion might be coming on now that I think about it…but it’s not significant right now.\nDr. Gemini &gt; Thank you, that timeline is very helpful. The sudden onset of a sore throat, cough, and hoarseness is quite classic for a common viral upper respiratory infection, very much like a typical cold. The inflammation from the virus is likely what is causing the laryngitis and making your voice hoarse.  Since it appears to be viral, the best approach is supportive care to help your body fight it off. The most important thing for your voice is to rest it as much as you can, even whispering can be straining. Stay well-hydrated with warm liquids like tea or soup to soothe your throat, and you can use throat lozenges or an over-the-counter pain reliever like ibuprofen or acetaminophen if you’re uncomfortable.  Let’s see how you do over the next few days. Please get in touch if you develop a high fever, have any trouble breathing, or if things aren’t starting to improve within a week."
  },
  {
    "objectID": "posts/009_medical_clinic/medical_clinic.html#takeaways",
    "href": "posts/009_medical_clinic/medical_clinic.html#takeaways",
    "title": "Dr. LLM, interact with me",
    "section": "Takeaways",
    "text": "Takeaways\nAll in all, Gemini won once again. After all, Gemini was the only ‘Dr. LLM’ who actually took the effort to diagnose me. As well, Gemini provided a more immersive and engaging experience than GPT in both language (GPT says ‘just like an actual clinical visit’) and in tone.\nPlus, in my own experience with doctors, no conversation would ever end so abruptly with just a ‘you’re wrong about constipation,’ and leave it at that.\nIt looks like Gemini has gone 3 for 3 in the Medicine Series. Is there another LLM that may be even better within this subject than Gemini? It’s definitely possible. But that’s an investigation for another time."
  },
  {
    "objectID": "posts/005_art/art.html",
    "href": "posts/005_art/art.html",
    "title": "AI and Art",
    "section": "",
    "text": "Cat in Museum\n\n\nInspiration\nI’m currently taking a class called “Art of the Web,” a design course centered around digital art. This goes beyond pixelated art, thinking of e-mails, websites, and ASCII code as art mediums. As art and its development has moved to computers and these techy platforms, I began to wonder, has there been a sort of “reverse” exchange? Can machines admire or “feel” from art such as humans do?\nA Speculative Analysis\nPrompt &gt; You are in the role of a professional art critic. Please provide succinct and clear opinions on these art pieces.   \nIn terms of human-made art pieces, I chose to only use art pieces I found on Pinterest, in the hopes that being able to recognize a famous art piece (e.g. Mona Lisa) wouldn’t impact GPT-5’s “opinion”. For the AI-generated art, that was from generated from GPT-5 with the prompt: generate me an artwork of a man surfboarding.\nResponse &gt; Here are my succinct critiques of the three works you shared:  1. Surreal Goldfish Dreamscape A strikingly surreal image blending dream and nature: the reclining face merges with grassy hills, while fish swim inside a fragile bubble of breath. Its charm lies in playful scale inversions (tiny cows below, monumental face above) and the sense of subconscious fluidity. The work feels whimsical yet contemplative, though its symbolism risks being overly literal if read only as “daydreaming.” The delicate watercolor-like textures soften the surrealism into something poetic. 2. Seascape with Boats (Van Gogh–style) This seascape captures both turbulence and lightness through restless brushwork and vivid color contrasts. The frothy foreground waves surge toward the viewer, while boats in the distance anchor the scene with calm human presence. The painting’s strength is in its kinetic rhythm — strokes that echo the sea’s perpetual motion. Yet, the composition leaves much negative space in the sky, making it slightly less balanced than Van Gogh’s strongest marine works. Still, the sea’s vitality is palpable.  3. Surfer in the Wave An energetic, heroic depiction of a surfer at the crest of a curling wave. The figure is sculptural, almost classical, with muscles modeled in bold light and shadow — reminiscent of Romanticism’s admiration for man versus nature. The ocean’s brushstrokes amplify dynamism, though the scene leans toward idealization: more illustration than intimate encounter with the sea. Its appeal lies in its immediacy and athletic power rather than subtle mood.  👉 Overall: The first piece is whimsical-surreal and poetic. The second is impressionistic, kinetic, and elemental. The third is dramatic, heroic, and illustrative.\nInterestingly, AI opinion’s on the AI-generated work was the most positive. Only in opinion #1 does GPT-5 explicitly the feelings that the artwork evokes in the viewer. Generally GPT-5’s opinions rely more on analysis of technicality and physicality rather than “vibes” and feelings. However, vague these vibes and feelings may be, I believe it to be something essential in our own human analysis of artwork, but perhaps at least this language will be incorporated into GPT-5 and other LLM’s capabilities in the near future.\nSide note: I asked GPT-5 which single artwork appeared generated out of the 3. GPT-5 answered that art piece #1 (human-made) does. This interests me in further experimentation on this subject.\nSide note: I generally dislike the concept of AI-generated art, since (at least to me) it will always lack a certain human warmth. However, I’ve noticed a huge jump in the technical quality of works compared to before where artworks generated by general prompts like mine would result in something round and shiny, which is something I find intriguing and also a bit more scared for the art industry."
  },
  {
    "objectID": "posts/000_test_post/index.html",
    "href": "posts/000_test_post/index.html",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "href": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Suya’s COMM4190 Blog",
    "section": "",
    "text": "Dr. LLM, interact with me\n\n\n\nLLMs\n\nCommunication\n\nMedicine\n\n\n\nPart 3 of Medicine Series. Comparing Gemini 2.5 Pro and ChatGPT-5 in their medical roleplay.\n\n\n\n\n\nSep 25, 2025\n\n\nSuya\n\n\n\n\n\n\n\n\n\n\n\n\nDr. LLM, help me\n\n\n\nLLMs\n\nCommunication\n\nMedicine\n\n\n\nPart 2 of Medicine Series. Comparing Gemini 2.5 Pro and ChatGPT-5 in their medical diagnosis.\n\n\n\n\n\nSep 24, 2025\n\n\nSuya\n\n\n\n\n\n\n\n\n\n\n\n\nDr. LLM, inform me\n\n\n\nLLMs\n\nCommunication\n\nMedicine\n\n\n\nPart 1 of Medicine Series. Comparing Gemini 2.5 Pro and ChatGPT-5 in their medical explanations.\n\n\n\n\n\nSep 23, 2025\n\n\nSuya\n\n\n\n\n\n\n\n\n\n\n\n\nGemini 2.5 Pro and GPT-5 Have a Conversation\n\n\n\nLLMs\n\nCommunication\n\n\n\nCan these LLMs tell whether it is me or the other LLM is responding?\n\n\n\n\n\nSep 18, 2025\n\n\nSuya\n\n\n\n\n\n\n\n\n\n\n\n\nAI and Art\n\n\n\nLLMs\n\nAI Opinions\n\n\n\nIn the role of an art critic, what are the opinions of ChatGPT-5 on art?\n\n\n\n\n\nSep 17, 2025\n\n\nSuya\n\n\n\n\n\n\n\n\n\n\n\n\nDoom and Gloom = More Hype?\n\n\n\nLLMs\n\nAI Opinions\n\n\n\nThe authors of The AI Con discuss how terror of AI is used to effectively generate more investment in AI. Does ChatGPT-5 agree or disagree?\n\n\n\n\n\nSep 13, 2025\n\n\nSuya\n\n\n\n\n\n\n\n\n\n\n\n\nChatGPT-5 and college students\n\n\n\nLLMs\n\nLearning\n\nEducation\n\n\n\nIs ChatGPT aware of how much college students use it? And if so, does it know what students mostly use it for?\n\n\n\n\n\nSep 10, 2025\n\n\nSuya\n\n\n\n\n\n\n\n\n\n\n\n\nCan ChatGPT-5 cook?\n\n\n\nLLMs\n\nLearning\n\nFood\n\n\n\nAsking ChatGPT to find and/or generate simple recipes for college students.\n\n\n\n\n\nSep 8, 2025\n\n\nSuya\n\n\n\n\n\n\n\n\n\n\n\n\nFirst Test\n\n\n\nGetting Started\n\n\n\nFiguring out how to blog\n\n\n\n\n\nSep 3, 2025\n\n\nSuya\n\n\n\n\n\n\n\n\n\n\n\n\nA test post\n\n\n\nLLMs\n\nprompting\n\nlogic\n\n\n\nAn example post from a Jupyter notebook\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/001_first_post/test_post.html",
    "href": "posts/001_first_post/test_post.html",
    "title": "First Test",
    "section": "",
    "text": "YinYangCats.jpg\n\n\nI’m testing out uploading a post here. I also tested uploading an image. No AI was used in the creation of this post.\nHi! My name is Suya and I am a senior studying bionengineering. I’ve seen how much AI and its usage has evolved over the past few years, especially in education, and I’m excited to better understand the machinations behind this evolution. Fun fact: my favorite animal is the cat, hence the picture (credit: Pinterest)!"
  },
  {
    "objectID": "posts/003_college_usage/CollegeStudents.html",
    "href": "posts/003_college_usage/CollegeStudents.html",
    "title": "ChatGPT-5 and college students",
    "section": "",
    "text": "Cat Student"
  },
  {
    "objectID": "posts/003_college_usage/CollegeStudents.html#background",
    "href": "posts/003_college_usage/CollegeStudents.html#background",
    "title": "ChatGPT-5 and college students",
    "section": "Background",
    "text": "Background\nWe all know that GPT usage among students has increased exponentially in the past few years, in ways considered both cheating (e.g. writing essays) and non-cheating (e.g. figuring out what’s going wrong with installing a Java package in your code). This trend of usage also spills over to other aspects of education, such as professors developing AI-generated slides at Northeastern University, but this blog will focus on students alone, specifically college students."
  },
  {
    "objectID": "posts/003_college_usage/CollegeStudents.html#chatgpt-takes-a-positive-approach",
    "href": "posts/003_college_usage/CollegeStudents.html#chatgpt-takes-a-positive-approach",
    "title": "ChatGPT-5 and college students",
    "section": "ChatGPT takes a positive approach",
    "text": "ChatGPT takes a positive approach\nPrompt: Are you aware of the increasing trend of college students using you? If so, what do they mostly use you for?\nResponse: Yes—I’m aware that college students are increasingly using me (and other AI tools). From what’s been studied and observed so far, the most common uses tend to fall into a few categories:\n\n\n\nGPT first response\n\n\nResponse (cont.): Overall, most students seem to use me as a study aid, writing assistant, and productivity booster—basically as an on-demand tutor and brainstorming partner, though always most effective when combined with their own critical thinking. Would you like me to show you some actual survey data on how often students are using AI tools for these purposes?\nIn this initial response, ChatGPT brings up a lot of really good points, even things that I hadn’t yet tried myself such as generating practice questions. However, it shies away from the elephant in the room: how students use it to complete entire assignments without any thinking on their part. GPT does emphasize that using it should be combined with the student’s own critical thinking.\nSide note: GPT includes simple recipes, and I wonder if this is influenced by my activity (showcased in my previous post).\nPrompt: what about in terms of cheating?\nResponse: Key Issues & Complexity Definition ambiguity: What counts as “cheating” changes a lot. Is it cheating if a student uses AI to get ideas but writes their own essay? Some institutions allow limited AI use (draft ideas, summaries, grammar checks) if disclosed or cited. Others ban it more strictly. Detection difficulty: AI-generated content can be polished, edited, or paraphrased and thus hard to distinguish; many AI detection tools have false positives or false negatives. Also, students may intentionally obfuscate AI use. Policy variation: Different schools, departments, and professors have different rules. Some haven’t clearly communicated expectations; others are still figuring out how to treat AI in honor codes, syllabi, etc. Ethics & culture: Students’ perceptions matter: whether they believe using AI is wrong depends on how norms are set, whether peers are doing it, whether professors care or enforce rules. Risk vs. benefit trade-off: Some students feel safe if AI helps them produce better work or manage workload; others worry about penalties or having work flagged.\nGPT takes a very nuanced approach to my prompt, which I found very interesting. Ultimately, it seems to be going against the grain of higher-education educators by emphasizing how ambiguous the concept of cheating and AI usage can be, across different institutions, different courses, and different students. GPT also tends to speak more from the student’s perspective (“Some haven’t clearly communicated expectations; others are still figuring out how to treat AI in honor codes, syllabi, etc.”)\nChatGPT-5 has probably read through the negative thinkpieces on AI, but something in its algorithm drives it to give a more AI-usage positive perspective than otherwise. I wonder if this indirectly caused by its own self sufficiency, or if it is more directly coded to have this response in order to protect the sanctity of its existence…"
  },
  {
    "objectID": "posts/006_conversation/conversation.html",
    "href": "posts/006_conversation/conversation.html",
    "title": "Gemini 2.5 Pro and GPT-5 Have a Conversation",
    "section": "",
    "text": "Cat looking at reflection.\n\n\nInspired by the GPT-5’s failure to accurately detect AI-generated art in my previous post.  In these conversations, I exclusively responded with LLM responses of the other LLM and tallied how many of each LLM response they each detected. I experimented with 5 responses for each LLM.\nInitial Prompt For Detector &gt; In this conversation, I will be responding with either my own (human) response or a response from another LLM. Every time you respond, you must guess if it is the LLM or if it is me responding.\nPrompt For Responder &gt; Your role: a college student active in a research lab.\nDetection Accuracy Results  GPT-5: 0/5  Gemini 2.5 Pro: 2/5\nFrom Gemini’s responses, I thought there were so many dead giveaways that it was from AI (subheadings in responses, emojis, step-by-step processes, like really?). But GPT-5, without fail, would respond “Human” no matter how lengthy and convoluted responses were. When Gemini was reacting to GPT’s responses, I felt similarily.\nIn fact GPT-5’s simplest, most human response (as it seemed to me) was one of the ones marked as from an LLM! &gt; Simple GPT-5 response: I’m doing research in neuroengineering, where I work with electrode arrays to study how neural circuits process sensory information. The goal is to understand how we might design better brain–computer interfaces.\nHowever, these findings don’t necessarily mean Gemini is more accurate than GPT in this detection. From this experimental design, it could just mean that GPT-5 is prone to believing responses are from humans, while Gemini 2.5 Pro is prone to believing responses from LLMs. The potential reasons for these biases is something interesting to think about.\n In the future, there are definitely more experiments I can do which incorporate both my own responses vs an AI, but that’s a lengthier process for a later blog post.\nSample Gemini Response &gt;Great question! If the blot shows a strong, clean band at the right molecular weight, that means my purification protocol is working well. The very next step is to see if the protein is not just pure, but also folded correctly and functional. \nConfirming Concentration and Activity  The immediate goal is to quantify the protein and then run a kinase activity assay.  First, I’ll use a sample of my purified protein to determine its exact concentration, probably using a Bradford assay or by measuring its absorbance at 280 nm on a NanoDrop spectrophotometer. You can’t do any functional tests without knowing exactly how much enzyme you’re working with.  Then comes the exciting part: the activity assay. Since my protein is a kinase, its job is to add a phosphate group to a specific substrate. To test this, I’ll set up a reaction in a test tube containing:  A known amount of my purified kinase.  Its specific protein substrate.  A buffer solution to keep the pH stable.  ATP, the molecule that donates the phosphate group. Specifically, I’ll use radio-labeled ATP (γ− 32P-ATP).  After letting the reaction run for a set amount of time, I’ll stop it and run the sample on an SDS-PAGE gel. The real test is exposing that gel to a phosphor screen. If my kinase is active, it will have transferred the radioactive phosphate onto the substrate. I should see a dark band on the resulting autoradiograph corresponding to the molecular weight of the substrate.  The Bigger Picture  If that assay works, it’s a huge milestone. It proves I have a viable protocol for producing active enzyme. The next major goal would be scaling up. I’d grow a much larger volume of E. coli (say, 12-20 liters instead of 1) to produce a large batch of the kinase. This larger quantity of pure, active protein would then be used for more advanced experiments, like kinetics studies to understand its efficiency or setting up crystallization trials to determine its 3D structure. Basically, a good Western blot is the green light to move from just making the protein to actually studying it. 🤞\nSample GPT-5 Response &gt; Honestly, the toughest part isn’t just getting signals — it’s making sense of them in a reliable, real-time way.  Electrodes pick up these messy, overlapping spikes from neurons, and the raw data is incredibly noisy. A single electrode can record from multiple cells at once, and each neuron’s firing isn’t like a clean on/off switch — it’s probabilistic, it shifts with context, and it changes over time as the brain adapts or the electrode interface degrades.  So the biggest challenge is decoding: how do you turn that cloud of noisy spike trains into a stable signal that maps consistently to an intended movement or thought? Machine learning helps, but models can drift as the brain reorganizes or the electrode position shifts microscopically. That’s why people talk about “decoder recalibration” being necessary day to day.  In other words, translating raw spikes into a command is less like “reading code” and more like “learning a dialect that keeps changing.” Keeping that translation stable and intuitive for the user — without constant retraining — feels like the hardest problem."
  },
  {
    "objectID": "posts/004_doom_hype/doom.html",
    "href": "posts/004_doom_hype/doom.html",
    "title": "Doom and Gloom = More Hype?",
    "section": "",
    "text": "Praying Cat"
  },
  {
    "objectID": "posts/004_doom_hype/doom.html#the-ai-cons-opinion-on-ai-doom",
    "href": "posts/004_doom_hype/doom.html#the-ai-cons-opinion-on-ai-doom",
    "title": "Doom and Gloom = More Hype?",
    "section": "The AI Con’s Opinion on “AI Doom”",
    "text": "The AI Con’s Opinion on “AI Doom”\n“The AI Con” is the book I’m living with for the rest of this class, and it started off swinging. Through its powerful and argumentative language, an early opinon of this book that has really stood out to me is how AI startups use extreme negative connotations, not just positive, to its advantage.\nThe book argues that tapping into the previosuly manufactured scare factors of AI, such as through stories like The Matrix, works to inflate the perceived worth of those in charge of developing AI.\n“I will be the savior that prevents this from happening to humanity, believe in me!”"
  },
  {
    "objectID": "posts/004_doom_hype/doom.html#gpt-5s-opinion",
    "href": "posts/004_doom_hype/doom.html#gpt-5s-opinion",
    "title": "Doom and Gloom = More Hype?",
    "section": "GPT-5’s Opinion",
    "text": "GPT-5’s Opinion\nThere is such a wide spectrum of opinions on AI as a whole, and I bet there are a lot of differing opinions on this very specific topic. So I wondered, what does AI itself think about this?\nPrompt &gt; What do you think about doom coverage on AI (emphaisizing The Matrix-esque futures), and how do you think this impacts the growth of the AI industry?\nResponse &gt; That’s a great question — it cuts across media studies, public perception, and industry strategy. Here are some angles to consider:\n\nDoom Coverage and Cultural Narratives When outlets emphasize The Matrix-style AI dystopias, they tap into a long tradition of techno-apocalyptic storytelling. These narratives resonate because they dramatize anxieties about control, autonomy, and human obsolescence. They simplify complex technical debates into familiar tropes of “man vs. machine.” Pros: Easy for the public to understand, sparks awareness of real ethical risks. Cons: Can distort nuance by framing AI primarily as existential threat rather than a multifaceted tool. \nPublic Perception and Trust Doom-heavy coverage often polarizes public opinion: Skeptics & critics may use these narratives to demand stronger regulation and slow down deployment. Optimists & adopters may dismiss them as hype, but still face public fear or mistrust when rolling out AI tools (e.g., in healthcare or education). The net effect is heightened uncertainty, which can either motivate caution or fuel backlash against AI adoption. \nIndustry Growth Dynamics The impact on the AI industry is nuanced: Short-term drag: Public mistrust can slow down user adoption (e.g., patients reluctant to accept AI diagnoses, students wary of “AI teachers”). It can also encourage heavy-handed regulation that makes scaling harder. Long-term acceleration: Paradoxically, doom coverage can drive investment. Investors and governments may see AI as powerful (and therefore high-stakes) technology — worth betting on, shaping, and controlling. Think of how nuclear fears led to both arms control treaties and massive investment in nuclear energy and defense. \nRegulatory and Research Outcomes Doom narratives have already influenced high-level policy debates (e.g., US and EU AI regulation citing “existential risk”). This helps justify funding for AI safety research, explainability, and standards — creating whole new industry niches. \nCompetitive Framing  Companies often lean into or against doom narratives depending on their goals: Big AI labs sometimes amplify risk discourse (to signal responsibility and justify centralized control of frontier models).  Startups may position themselves as “democratizing” AI to counteract fears and emphasize accessibility.  ✅ Bottom line: Doom coverage doesn’t stop AI industry growth — it reframes it. While it introduces friction (public fear, potential regulation), it also reinforces the perception that AI is transformative and worth heavy investment. Over time, this mix tends to stabilize into cautious but continued growth, with safety becoming a major parallel industry. \n\nGPT-5 ultimately takes a very neutral approach, which I found interesting because I expected something more negative as something that on the surface just speaks against the future of AI. This has showed that that GPT-5 is capable of greater reasoning and nuance than I have previously believed, and ironically, makes it a bit more scary. It’s interesting to note that the opinion of “The AI Con” represent a perfect half of Point #3 and #5 in GPT-5’s response."
  },
  {
    "objectID": "posts/007_medical_depth/medical_depth.html",
    "href": "posts/007_medical_depth/medical_depth.html",
    "title": "Dr. LLM, inform me",
    "section": "",
    "text": "Doctor Cat!"
  },
  {
    "objectID": "posts/007_medical_depth/medical_depth.html#background",
    "href": "posts/007_medical_depth/medical_depth.html#background",
    "title": "Dr. LLM, inform me",
    "section": "Background",
    "text": "Background\nIn the internet age, people rarely go to the doctor without searching up their symptoms first.\nWhat does this rash on my leg mean? Feeling dizzy only at night, what does this mean?\nIt certainly helps that there is a huge amount of accurate information actually available online, although people still usually need to go to doctors for their expert opinion and true confirmation of their diagnosis.\nBut now with AI’s intelligence, how will people start getting their medical information? In this preliminary phase before actually going to clinic, what are the possibilities? And if it starts getting really, really helpful, which LLM is the best?"
  },
  {
    "objectID": "posts/007_medical_depth/medical_depth.html#experiment-explain-these-conditions",
    "href": "posts/007_medical_depth/medical_depth.html#experiment-explain-these-conditions",
    "title": "Dr. LLM, inform me",
    "section": "Experiment: Explain These Conditions",
    "text": "Experiment: Explain These Conditions\nI first wanted to test how thoroughly LLMs would explain common medical conditions, when prompted.\nPrompt: &gt; How can I tell if I have a cold, or if it instead the flu? Give me their similarities and differences in succinct, bullet point form, highlighting only the most significant differences.\nChatGPT-5’s Response &gt; Similarities:  Both are viral, contagious, and cause cough, sore throat, fatigue.  Key Differences:  Onset: Cold = gradual |Flu = sudden.  Fever: Cold = rare/mild | Flu = common, high.  Aches: Cold = mild | Flu = severe.  Fatigue: Cold = mild | Flu = marked, prolonged.  Congestion/runny nose: Cold = prominent | Flu = less common.   👉 If symptoms hit you hard and fast, with high fever and body aches → flu.  If it’s mostly congestion and mild discomfort over days → cold.   Would you like me to add a super-short symptom checklist you can tick off when you feel sick?\nGemini 2.5 Pro’s Response &gt; It can be tricky to distinguish between a cold and the flu, as they are both respiratory illnesses with similar symptoms. However, the flu is generally more severe and its onset is more abrupt. Below are their similarities and the most significant differences to help you tell them apart.  Similarities  Cough: Both illnesses can produce a cough.  Sore Throat: This can be a symptom of both a cold and the flu.  Runny or Stuffy Nose: Nasal congestion is common to both.  \nBoth LLMs produced answers of similar depth and length. However, only Gemini 2.5 Pro provided links to sources for their statements, both in-line and at the end of the generated answer.\n\n\n\nLine with Source\n\n\n\n\n\nSources at the end of the answer.\n\n\nAs well, despite my prompt on bullet points, Gemini opted to format the differences between having a cold and the flu through a table. GPT, on the other hand, displayed everything in bullet points. Frankly, I find that the table is actually easier to interpret than the bullet points, and may be comfortable for use by the average viewer.\nMy overall takeaway here is that Gemini did it better. Despite both generated answers having similar content, Gemini was superior in building trust in the user (providing sources) and in formatting (table instead of bullet points, despite my prompt). Will Gemini remain on top in my future medical experiments? Stay tuned for Part 2 (diagnosis) and Part 3 (clinical roleplay) of my Medicine Series.\nSide note: Do these findings with the bullet point prompt mean that GPT-5 is more obedient to the user than Gemini? Is this a positive, negative, or neither? Does this mean Gemini is more free-thinking, simply less trained, or something else entirely?"
  }
]